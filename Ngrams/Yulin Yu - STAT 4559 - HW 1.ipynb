{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Have Fun With Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 On exercises from SLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 The set of all lower case alphabetic strings ending in a b;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = r'^[a-z]*b$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ^: starting point\n",
    "- [a-z]* : lower case alphabets that can have a length of 0 or more\n",
    "- b$ : string ends with a \"b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 3), match='lab'>\n",
      "<re.Match object; span=(0, 1), match='b'>\n"
     ]
    }
   ],
   "source": [
    "#test cases\n",
    "print(re.search(pattern1, '2ab')) #shouldn't work because it contains int\n",
    "print(re.search(pattern1, 'Lab')) #shouldn't work because of the upper case 'L'\n",
    "print(re.search(pattern1, 'lab')) #should work\n",
    "print(re.search(pattern1, 'b')) #should work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 The set of all strings from the alphabet a, b such that each a is immediately preceded by and immediately followed by a b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = '^(b*ba)*bb*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ^: starting point\n",
    "- ba: \"a\" is immediately preceded by a \"b\"\n",
    "- the first * : it doesn't matter how many \"b\"s are before a\n",
    "- the second * : it doesn't matter if a exists\n",
    "- the \"b\" after the second * : \"a\" is immediately followed by a \"b\"\n",
    "- the b* at the end: it doesn't matter how many \"b\"s are after a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 1), match='b'>\n",
      "<re.Match object; span=(0, 3), match='bab'>\n",
      "<re.Match object; span=(0, 23), match='bbbabbbbabbabbbbbbbbabb'>\n"
     ]
    }
   ],
   "source": [
    "#test cases\n",
    "print(re.search(pattern2, 'aba')) #shouldn't work because a is not preceded by b\n",
    "print(re.search(pattern2, 'b')) #should work\n",
    "print(re.search(pattern2, 'bab')) #should work\n",
    "print(re.search(pattern2, 'bbbabbbbabbabbbbbbbbabb'))#should work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 The set of all strings with two consecutive repeated words (e.g., “Humbert Humbert” and “the the” but not “the bug” or “the big bug”);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern3 = r'([a-zA-Z]+)\\s+\\1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ([a-zA-Z]+) : a stored word\n",
    "- \\s+: spaces\n",
    "- \\1: repetition of the stored word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(4, 11), match='big big'>\n"
     ]
    }
   ],
   "source": [
    "#test cases\n",
    "print(re.search(pattern3, 'the bug')) #shouldn't work; no repetition\n",
    "print(re.search(pattern3, 'the big big bug')) #should work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 All strings that start at the beginning of the line with an integer and that end at the end of the line with a word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern4 = r'^\\d+[^\\n]*[a-zA-Z]+$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ^\\d+: begining of the line with an integer\n",
    "- [^\\n]*: anything except a new line\n",
    "- [a-zA-Z]+$: end the line with a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 15), match='4559@#%$$%^STAT'>\n"
     ]
    }
   ],
   "source": [
    "#test cases\n",
    "print(re.search(pattern4, ' 4559 STAT ')) #shouldn't work; it starts and ends with space\n",
    "print(re.search(pattern4, '4559@#%$$%\\n^STAT')) #shouldn't work ; contains a new line\n",
    "print(re.search(pattern4, '4559@#%$$%^STAT')) #should work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 On a regex for phone numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_number = r'^((\\d{3}-)|(\\(\\d{3})\\)\\s)?(\\d{3})-(\\d{4})$'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ((\\d{3}-)|(\\(\\d{3})\\)\\s) : either (xxx) or xxx-\n",
    "- ?: The area code can be ignored\n",
    "- (\\d{3})-(\\d{4}): the seven number digit\n",
    "- ^ and $: match the pattern strictly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 14), match='(123) 456-7890'>\n",
      "<re.Match object; span=(0, 12), match='123-456-7890'>\n",
      "<re.Match object; span=(0, 8), match='456-7890'>\n"
     ]
    }
   ],
   "source": [
    "#test cases \n",
    "\n",
    "print(re.search(phone_number, '-456-7890')) #shouldn't work; not a phone number \n",
    "\n",
    "#all should work below\n",
    "print(re.search(phone_number, '(123) 456-7890')) \n",
    "print(re.search(phone_number, '123-456-7890')) \n",
    "print(re.search(phone_number, '456-7890')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 On a regex for reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = r'UVA_(5[0-9]|60)_(\\d{3})_F_90_(uninj|inj)_(y|n)\\.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file pattern is based on the format of the file names. Specifically, \n",
    "- UVA: school name\n",
    "- (5[0-9]|60): age range [50,60]\n",
    "- (\\d{3}): the id consists of 3 digits according to the files.\n",
    "- F: female only as required\n",
    "- 90: survey 90 as required\n",
    "- (uninj|inj): it doesn't matter whether the person is injured or not.\n",
    "- (y|n): it doesn't matter yes or no to the question.\n",
    "\n",
    "Now let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UVA_50_850_F_90_uninj_n.csv\n",
      "UVA_50_476_F_90_uninj_n.csv\n",
      "UVA_50_436_F_90_inj_n.csv\n",
      "UVA_59_351_F_90_inj_y.csv\n",
      "UVA_56_975_F_90_inj_y.csv\n",
      "UVA_52_678_F_90_uninj_y.csv\n",
      "UVA_59_167_F_90_inj_n.csv\n",
      "UVA_58_684_F_90_inj_y.csv\n",
      "UVA_57_462_F_90_inj_n.csv\n",
      "UVA_53_542_F_90_uninj_y.csv\n",
      "UVA_58_760_F_90_inj_n.csv\n",
      "UVA_53_464_F_90_uninj_n.csv\n",
      "UVA_60_352_F_90_uninj_y.csv\n",
      "UVA_54_149_F_90_uninj_y.csv\n",
      "UVA_60_297_F_90_inj_y.csv\n",
      "UVA_50_990_F_90_inj_y.csv\n",
      "UVA_60_285_F_90_uninj_n.csv\n",
      "UVA_58_635_F_90_inj_y.csv\n",
      "UVA_59_423_F_90_inj_y.csv\n",
      "UVA_50_681_F_90_uninj_n.csv\n",
      "UVA_56_498_F_90_inj_n.csv\n",
      "UVA_55_741_F_90_inj_y.csv\n",
      "UVA_59_479_F_90_uninj_y.csv\n",
      "UVA_58_834_F_90_inj_n.csv\n",
      "UVA_59_795_F_90_inj_n.csv\n",
      "UVA_56_759_F_90_inj_n.csv\n",
      "UVA_52_107_F_90_uninj_y.csv\n",
      "UVA_58_198_F_90_inj_y.csv\n",
      "UVA_57_617_F_90_inj_n.csv\n",
      "Total number of files found: 29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir('./regex_files')\n",
    "ct = 0\n",
    "for filename in files:\n",
    "    if re.search(file_pattern, filename) != None:\n",
    "        ct+=1\n",
    "        print(filename)\n",
    "print(f'Total number of files found: {ct}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Our First Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bigrams =  pd.read_csv('bigrams.csv')\n",
    "unigram = pd.read_csv('unigrams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 On the dataset side: discuss the potential pitfalls with using this specific data set to train the bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, by using only the first 80 sentences of the book as corpus, we are more than likely to get a biased language model because the corpus is very likely not to contain all possible words, especially words appeared not in literature but in sports, medication, engineering, law, etc. \n",
    "\n",
    "One of the consequences is that the model will be subject to **the problem of sparsity**. Because the corpus is limited, words that do occur in the test set do not occur in the training set, and when we apply the model based on such corpus to test data, we will have many cases of putative zero probability bigrams that should really have non-zero probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we have the following bigrams in the corpus:\n",
    "\n",
    "- I said\n",
    "- said what\n",
    "- what are\n",
    "- are you\n",
    "- you doing\n",
    "\n",
    "And suppose our test set has a phrase: \n",
    "\n",
    "- You said\n",
    "\n",
    "Our model will incorrectly estimate the probability P(You said) = P(said|you)* P(you) = 0* 2/5 = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's more, we will get wrong estimation for sentences if the probability of any bigrams in the test set is 0 because the probability of a sentence can be factorized into products of probabilities of bigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the sparisity is about the problem of words whose bigram probability is zero, The **out of vocabulary(OOV)** problem is that we may have to deal with words we haven’t seen before because of the limited corpus. There are several solutions to solve this problem, such as inserting them OOV as pseudo-words \"UNK\" into training data and replacing words in the training set \"UNK\" based on their frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 On the probability of sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Maximum likelihood probability of a bigram is as follow:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:bigrams}\n",
    "    P(W_{n}|W_{n-1})  = \\frac{C(W_{n-1} W_{n})}{C(W_{n-1})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:bigrams2}\n",
    "    P(W_{n-1} W_{n})  = \\frac{C(W_{n-1} W_{n})}{C(Bigrams)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that `C(W_n-1)` is different when using bigrams or unigrams. For example, using unigrams, we get count('not') = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.loc[unigram['unigram']=='not','count'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we use bigrams, we get count('not') = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams.loc[(bigrams['word1']=='not'),'count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is caused by the fact that our bigrams do not conclude the instance of (word, $) so the last words did not get counted as the first word of a bigram.  \n",
    "\n",
    "While the difference is very small for huge data, but in our case where the corpus is small, I will take `C(W_n-1)` as count of it appearances as the first word of a bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def sentence_prob(sentence):\n",
    "    prob = [] #store bigram probabilities\n",
    "    \n",
    "    #word tokenization\n",
    "    words =  word_tokenize(sentence)\n",
    "    lower_words = [x.lower() for x in words]\n",
    "    \n",
    "    #calculate joint probability for the first two words\n",
    "    logic1=((bigrams['word1']==lower_words[0])&(bigrams['word2']==lower_words[1]))\n",
    "    ct_w1_w2 = bigrams.loc[logic1,'count'].values[0]\n",
    "    total_bigrams = bigrams['count'].sum()\n",
    "    prob_w1_w2 = ct_w1_w2/total_bigrams\n",
    "    prob.append(prob_w1_w2)\n",
    "    \n",
    "    print(f'Count of ’{lower_words[0]} {lower_words[1]}’: {ct_w1_w2}')\n",
    "    print(f'Count of total bigrams: {total_bigrams}')\n",
    "    print(f'P({lower_words[0]} {lower_words[1]}) = {ct_w1_w2}/{total_bigrams} = {prob_w1_w2}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    #calculate the conditional probability of the rest of the words\n",
    "    for i in range(2,len(lower_words)):\n",
    "        #count of bigrams\n",
    "        logic2 = ((bigrams['word1']==lower_words[i-1])&(bigrams['word2']==lower_words[i]))\n",
    "        bigram_count = bigrams.loc[logic2,'count'].values\n",
    "        \n",
    "        if bigram_count.size == 0: #if the bigram is not in the corpus\n",
    "            bigram_count = 0 #count = 0\n",
    "        else:\n",
    "            bigram_count = bigram_count[0] #get the count\n",
    "        \n",
    "        unigram_count = bigrams.loc[bigrams['word1']==lower_words[i-1],'count'].sum() \n",
    "        prob.append(bigram_count/unigram_count) #calculate bigram probability\n",
    "        sentence_prob = np.prod(prob) #calculate sentence probability\n",
    "            \n",
    "        print(f'Count of ’{lower_words[i-1]} {lower_words[i]}‘: {bigram_count}')\n",
    "        print(f'Count of ’{lower_words[i-1]}‘: {unigram_count}')\n",
    "        print(f'P({lower_words[i]}|{lower_words[i-1]}) = {bigram_count}/{unigram_count} = {bigram_count/unigram_count}')\n",
    "        print('--------------------------------')\n",
    "        \n",
    "    print(f'Probabilty of the Sentence = product of each probability above = {sentence_prob}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ”It is not a good word for that\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ’it is’: 3\n",
      "Count of total bigrams: 547\n",
      "P(it is) = 3/547 = 0.005484460694698354\n",
      "--------------------------------\n",
      "Count of ’is not‘: 2\n",
      "Count of ’is‘: 10\n",
      "P(not|is) = 2/10 = 0.2\n",
      "--------------------------------\n",
      "Count of ’not a‘: 1\n",
      "Count of ’not‘: 5\n",
      "P(a|not) = 1/5 = 0.2\n",
      "--------------------------------\n",
      "Count of ’a good‘: 1\n",
      "Count of ’a‘: 12\n",
      "P(good|a) = 1/12 = 0.08333333333333333\n",
      "--------------------------------\n",
      "Count of ’good word‘: 1\n",
      "Count of ’good‘: 2\n",
      "P(word|good) = 1/2 = 0.5\n",
      "--------------------------------\n",
      "Count of ’word for‘: 1\n",
      "Count of ’word‘: 1\n",
      "P(for|word) = 1/1 = 1.0\n",
      "--------------------------------\n",
      "Count of ’for that‘: 1\n",
      "Count of ’for‘: 12\n",
      "P(that|for) = 1/12 = 0.08333333333333333\n",
      "--------------------------------\n",
      "Probabilty of the Sentence = product of each probability above = 7.617306520414382e-07\n"
     ]
    }
   ],
   "source": [
    "sentence_prob('It is not a good word for that')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.  ”You must indeed go for your own good”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ’you must’: 4\n",
      "Count of total bigrams: 547\n",
      "P(you must) = 4/547 = 0.007312614259597806\n",
      "--------------------------------\n",
      "Count of ’must indeed‘: 1\n",
      "Count of ’must‘: 6\n",
      "P(indeed|must) = 1/6 = 0.16666666666666666\n",
      "--------------------------------\n",
      "Count of ’indeed go‘: 1\n",
      "Count of ’indeed‘: 2\n",
      "P(go|indeed) = 1/2 = 0.5\n",
      "--------------------------------\n",
      "Count of ’go for‘: 1\n",
      "Count of ’go‘: 4\n",
      "P(for|go) = 1/4 = 0.25\n",
      "--------------------------------\n",
      "Count of ’for your‘: 1\n",
      "Count of ’for‘: 12\n",
      "P(your|for) = 1/12 = 0.08333333333333333\n",
      "--------------------------------\n",
      "Count of ’your own‘: 1\n",
      "Count of ’your‘: 3\n",
      "P(own|your) = 1/3 = 0.3333333333333333\n",
      "--------------------------------\n",
      "Count of ’own good‘: 0\n",
      "Count of ’own‘: 2\n",
      "P(good|own) = 0/2 = 0.0\n",
      "--------------------------------\n",
      "Probabilty of the Sentence = product of each probability above = 0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_prob('You must indeed go for your own good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.  ”How can you mistake flatter me”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ’how can’: 3\n",
      "Count of total bigrams: 547\n",
      "P(how can) = 3/547 = 0.005484460694698354\n",
      "--------------------------------\n",
      "Count of ’can you‘: 2\n",
      "Count of ’can‘: 3\n",
      "P(you|can) = 2/3 = 0.6666666666666666\n",
      "--------------------------------\n",
      "Count of ’you mistake‘: 1\n",
      "Count of ’you‘: 22\n",
      "P(mistake|you) = 1/22 = 0.045454545454545456\n",
      "--------------------------------\n",
      "Count of ’mistake flatter‘: 0\n",
      "Count of ’mistake‘: 1\n",
      "P(flatter|mistake) = 0/1 = 0.0\n",
      "--------------------------------\n",
      "Count of ’flatter me‘: 1\n",
      "Count of ’flatter‘: 1\n",
      "P(me|flatter) = 1/1 = 1.0\n",
      "--------------------------------\n",
      "Probabilty of the Sentence = product of each probability above = 0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_prob('How can you mistake flatter me')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.  ”Of them much to be for my dear”**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of ’of them’: 5\n",
      "Count of total bigrams: 547\n",
      "P(of them) = 5/547 = 0.009140767824497258\n",
      "--------------------------------\n",
      "Count of ’them much‘: 1\n",
      "Count of ’them‘: 6\n",
      "P(much|them) = 1/6 = 0.16666666666666666\n",
      "--------------------------------\n",
      "Count of ’much to‘: 1\n",
      "Count of ’much‘: 2\n",
      "P(to|much) = 1/2 = 0.5\n",
      "--------------------------------\n",
      "Count of ’to be‘: 2\n",
      "Count of ’to‘: 13\n",
      "P(be|to) = 2/13 = 0.15384615384615385\n",
      "--------------------------------\n",
      "Count of ’be for‘: 1\n",
      "Count of ’be‘: 8\n",
      "P(for|be) = 1/8 = 0.125\n",
      "--------------------------------\n",
      "Count of ’for my‘: 2\n",
      "Count of ’for‘: 12\n",
      "P(my|for) = 2/12 = 0.16666666666666666\n",
      "--------------------------------\n",
      "Count of ’my dear‘: 6\n",
      "Count of ’my‘: 9\n",
      "P(dear|my) = 6/9 = 0.6666666666666666\n",
      "--------------------------------\n",
      "Probabilty of the Sentence = product of each probability above = 1.6276295983791412e-06\n"
     ]
    }
   ],
   "source": [
    "sentence_prob('Of them much to be for my dear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 On the interpretation of these probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zero probabilities of sentence 2 and 3 illustrate the problem of sparsity. By observing the bigram count of each component of the sentences, we can see that the bigram \"own good\" for sentence 2 and \"mistake flatter\" for sentence 3 are not in the corpus and thus have count of zero. As a result, the bigram probability and sentence probability also become zero.\n",
    "\n",
    "We can also see that probabilities of sentence 1 and 4 are non-zero. It is normal for sentence 1 since it is grammatically correct, but abnormal for sentence 4 that consists of words in random orders. This is the problem with using a bigram model: each word is contingent on one previous word only without capturing the comprehensive context. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Laplace (Add One) Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the Maximum Likelihood Estimation of bigrams:\n",
    "\\begin{equation}\n",
    "\\label{eq:bigramsp}\n",
    "    P(W_{n}|W_{n-1})  = \\frac{C(W_{n-1} W_{n})}{C(W_{n-1})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:bigramsp1}\n",
    "    P(W_{n-1} W_{n})  = \\frac{C(W_{n-1} W_{n})}{C(Bigrams)}\n",
    "\\end{equation}\n",
    "\n",
    "Now, with Laplace add-one smoothing, the count of each bigram `P(W_{n-1} W_{n})` is added by one, and each `W_{n-1}` that appears as the first entry in a bigram is needed to increase by V, which is the unique number of unigrams.\n",
    "\n",
    "For example, before smoothing, P(word V |word2) = 2 /sum(column of word 2):\n",
    "\n",
    "\n",
    "|           | word 1 | word 2 | \\.\\.\\. | word V\\-1 | word V |\n",
    "|-----------|--------|--------|--------|-----------|--------|\n",
    "| word 1    | 1      | 2      | \\.\\.\\. | 0         | 0      |\n",
    "| word 2    | 0      | 5      | \\.\\.\\. | 1         | 0      |\n",
    "| \\.\\.\\.    | \\.\\.\\. | \\.\\.\\. | \\.\\.\\. | \\.\\.\\.    | \\.\\.\\. |\n",
    "| word V\\-1 | 7      | 0      | \\.\\.\\. | 0         | 1      |\n",
    "| word V    | 8      | 2      | \\.\\.\\. | 3         | 2      |\n",
    "\n",
    "After smoothing, each cell is increased by 1, so P(word V |word2) = 2+1 /sum(column of word 2)+V:\n",
    "\n",
    "\n",
    "|           | word 1 | word 2 | \\.\\.\\. | word V\\-1 | word V |\n",
    "|-----------|--------|--------|--------|-----------|--------|\n",
    "| word 1    | 1+1      | 2+1      | \\.\\.\\.+1 | 0+1         | 0+1      |\n",
    "| word 2    | 0+1      | 5+1      | \\.\\.\\.+1 | 1+1         | 0+1      |\n",
    "| \\.\\.\\.    | \\.\\.\\.+1 | \\.\\.\\.+1 | \\.\\.\\.+1 | \\.\\.\\.+1    | \\.\\.\\.+1 |\n",
    "| word V\\-1 | 7+1      | 0+1      | \\.\\.\\.+1 | 0+1         | 1+1      |\n",
    "| word V    | 8+1      | 2+1      | \\.\\.\\.+1 | 3+1         | 2+1      |\n",
    "\n",
    "\n",
    "Therefore, the formula for the conditional probability of bigram after smoothing is:\n",
    "\\begin{equation}\n",
    "\\label{eq:bigramsp2}\n",
    "P(W_{n}|W_{n-1}) = \\frac{C(W_{n-1} W_{n})+1}{C(W_{n-1})+V}\n",
    "\\end{equation}\n",
    "\n",
    "With total number of bigrams increases by V^2 (bigrams with V different first entries all increase by V), we have the joint probability:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:bigramsp4}\n",
    "P(W_{n-1} W_{n}) = \\frac{C(W_{n-1} W_{n})+1}{C(Bigrams)+V^{2}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Still, I will take `C(W_n-1)` as count of it appearances as the first word of a bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one_smoothing_prob(sentence, verbose=True):\n",
    "    prob = [] #store bigram probabilities\n",
    "    \n",
    "    #word tokenization\n",
    "    words =  word_tokenize(sentence)\n",
    "    lower_words = [x.lower() for x in words]\n",
    "    \n",
    "    #calculate oint probability  for the first two words\n",
    "    logic1= ((bigrams['word1']==lower_words[0])&(bigrams['word2']==lower_words[1]))\n",
    "    ct_w1_w2 =bigrams.loc[logic1,'count'].values[0] + 1\n",
    "    total_bigrams = bigrams['count'].sum() + (len(unigram))**2\n",
    "    prob_w1_w2 = ct_w1_w2/total_bigrams\n",
    "    prob.append(prob_w1_w2)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Count of ’{lower_words[0]} {lower_words[1]}’: {ct_w1_w2}')\n",
    "        print(f'Count of total bigrams: {total_bigrams}')\n",
    "        print(f'Probability of P({lower_words[0]} {lower_words[1]}) = {ct_w1_w2}/{total_bigrams} = {prob_w1_w2}')\n",
    "        print('--------------------------------')\n",
    "    \n",
    "    #calculate the conditional probability of the rest of the words\n",
    "    for i in range(2,len(lower_words)):\n",
    "        #count of bigram\n",
    "        logic2= ((bigrams['word1']==lower_words[i-1])&(bigrams['word2']==lower_words[i]))\n",
    "        bigram_count= bigrams.loc[logic2,'count'].values\n",
    "        \n",
    "        if bigram_count.size == 0: #if the bigram is not in the corpus\n",
    "            bigram_count = 1 #count = 1 with smoothing\n",
    "        else:\n",
    "            bigram_count = bigram_count[0]+1 #get the count from corpus +1\n",
    "         \n",
    "        #count of unigram\n",
    "        unigram_count = bigrams.loc[bigrams['word1']==lower_words[i-1],'count'].sum() + len(unigram)\n",
    "        prob.append(bigram_count/unigram_count) #calculate bigram probability\n",
    "        sentence_prob = np.prod(prob) #calculate sentence probability\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Count of ’{lower_words[i-1]} {lower_words[i]}‘: {bigram_count}')\n",
    "            print(f'Count of ’{lower_words[i-1]}‘: {unigram_count}')\n",
    "            print(f'P({lower_words[i]}|{lower_words[i-1]}) = {bigram_count}/{unigram_count} = {bigram_count/unigram_count}')\n",
    "            print('--------------------------------')\n",
    "        \n",
    "    print(f'Probabilty of \"{sentence}\" = {sentence_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe the new probabilities of the four sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilty of \"It is not a good word for that\" = 2.4778831051678218e-17\n"
     ]
    }
   ],
   "source": [
    "add_one_smoothing_prob('It is not a good word for that', False) #sentence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilty of \"You must indeed go for your own good\" = 1.0869167705142373e-17\n"
     ]
    }
   ],
   "source": [
    "add_one_smoothing_prob('You must indeed go for your own good', False) #sentence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilty of \"How can you mistake flatter me\" = 1.977589590025535e-13\n"
     ]
    }
   ],
   "source": [
    "add_one_smoothing_prob('How can you mistake flatter me',False) #sentence 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilty of \"Of them much to be for my dear\" = 1.8898708715474837e-16\n"
     ]
    }
   ],
   "source": [
    "add_one_smoothing_prob('Of them much to be for my dear',False) #sentence 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By redistributing counts from seen to unseen bigrams, the original zero-probability sentence 2 and 3 now have small positive probabilities, and sentence 1 and 4 with previously non-zero probabilities now have smaller probabilities. The sparsity problem is solved.\n",
    "\n",
    "However, the incapability of accurately capturing contexts associated with the bigram model is not resolved as the probability of sentence 4 is still non-zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
